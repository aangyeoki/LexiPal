{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': 0, 'fear': 1, 'joy': 2, 'sadness': 3}\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\LexiPal - AI\\.venv\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m711/711\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 73ms/step - accuracy: 0.4326 - loss: 1.1982\n",
      "Epoch 2/5\n",
      "\u001b[1m711/711\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 71ms/step - accuracy: 0.9147 - loss: 0.2722\n",
      "Epoch 3/5\n",
      "\u001b[1m711/711\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 68ms/step - accuracy: 0.9569 - loss: 0.1396\n",
      "Epoch 4/5\n",
      "\u001b[1m711/711\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 65ms/step - accuracy: 0.9643 - loss: 0.1003\n",
      "Epoch 5/5\n",
      "\u001b[1m711/711\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 59ms/step - accuracy: 0.9640 - loss: 0.0769\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step\n",
      "Predicted Emotion: joy\n"
     ]
    }
   ],
   "source": [
    "# Library\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('eng_dataset.csv')\n",
    "\n",
    "# Use 'content' for texts and 'sentiment' for labels\n",
    "texts = data['content'].values\n",
    "labels = data['sentiment'].values\n",
    "\n",
    "# Preprocessing texts\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=100)\n",
    "\n",
    "# Label encoding\n",
    "label_dict = {label: idx for idx, label in enumerate(sorted(set(labels)))}\n",
    "encoded_labels = np.array([label_dict[label] for label in labels])\n",
    "print(label_dict)\n",
    "\n",
    "# Model definition\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(label_dict), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences, encoded_labels, epochs=5, batch_size=10)\n",
    "\n",
    "# Save the model\n",
    "model.save('emotion_detection_model.keras')\n",
    "\n",
    "# Function to predict emotion\n",
    "def predict_emotion(text):\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(sequence, maxlen=100)\n",
    "    prediction = model.predict(padded)\n",
    "    emotion = list(label_dict.keys())[np.argmax(prediction)]\n",
    "    return emotion\n",
    "\n",
    "# Example usage\n",
    "example_text = \"I'm so happy to see this project working perfectly!\"\n",
    "predicted_emotion = predict_emotion(example_text)\n",
    "print(f\"Predicted Emotion: {predicted_emotion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run to export tokenizer\n",
    "# import json\n",
    "# tokenizer_json = tokenizer.to_json()  # Convert tokenizer to JSON\n",
    "# with open(\"tokenizer.json\", \"w\") as file:\n",
    "#     json.dump(tokenizer_json, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/en/ljspeech/tacotron2-DDC is already downloaded.\n",
      " > vocoder_models/en/ljspeech/hifigan_v2 is already downloaded.\n",
      " > Using model: Tacotron2\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\LexiPal - AI\\.venv\\lib\\site-packages\\TTS\\utils\\io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Model's reduction rate `r` is set to: 1\n",
      " > Vocoder Model: hifigan\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Generator Model: hifigan_generator\n",
      " > Discriminator Model: hifigan_discriminator\n",
      "Removing weight norm...\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from TTS.api import TTS\n",
    "\n",
    "# Load TTS model\n",
    "model = load_model('C:/LexiPal - AI/emotion_detection_model.keras')\n",
    "tts = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DDC\", progress_bar=False, gpu=False)\n",
    "\n",
    "# Ensure the tokenizer is properly loaded/trained\n",
    "# Example: tokenizer = ... (load or define your tokenizer here)\n",
    "\n",
    "# Ensure the output folder exists\n",
    "import os\n",
    "if not os.path.exists('static/audio'):\n",
    "    os.makedirs('static/audio')\n",
    "\n",
    "# Emotion prediction function\n",
    "def predict_emotion(text):\n",
    "    # Tokenizing and padding text\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(sequence, maxlen=100)\n",
    "    \n",
    "    # Predicting emotion using the trained model\n",
    "    prediction = model.predict(padded)\n",
    "    \n",
    "    # Get the predicted class index (highest probability)\n",
    "    predicted_class_index = np.argmax(prediction)\n",
    "\n",
    "    # Reverse the label_dict to map index to emotion label\n",
    "    emotion = [emotion for emotion, index in label_dict.items() if index == predicted_class_index][0]\n",
    "    \n",
    "    return emotion\n",
    "\n",
    "# Generate speech based on detected emotion\n",
    "def generate_speech(text, emotion, speaker_wav=None):\n",
    "    try:\n",
    "        emotion_text = f\"{text}\"\n",
    "        output_file = f\"static/audio/output_{emotion}.wav\"\n",
    "\n",
    "        if speaker_wav:\n",
    "            tts.tts_to_file(text=emotion_text, speaker_wav=speaker_wav, file_path=output_file)\n",
    "        else:\n",
    "            tts.tts_to_file(text=emotion_text, file_path=output_file)\n",
    "\n",
    "        print(f\"Generated speech with emotion '{emotion}' saved as '{output_file}'.\")\n",
    "        return output_file\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating speech: {e}\")\n",
    "        return None\n",
    "\n",
    "# Combine emotion detection and TTS\n",
    "def text_to_emotional_speech(text, speaker_wav=None):\n",
    "    # Step 1: Predict emotion\n",
    "    detected_emotion = predict_emotion(text)\n",
    "    print(f\"Detected Emotion: {detected_emotion}\")\n",
    "\n",
    "    # Step 2: Generate speech with detected emotion\n",
    "    output_file = generate_speech(text, detected_emotion, speaker_wav)\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step\n",
      "Detected Emotion: anger\n",
      " > Text splitted to sentences.\n",
      "['are you kidding me right now?!']\n",
      " > Processing time: 1.5115046501159668\n",
      " > Real-time factor: 0.699711906597604\n",
      "Generated speech with emotion 'anger' saved as 'static/audio/output_anger.wav'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'static/audio/output_anger.wav'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_emotional_speech(\"are you kidding me right now?!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Detected Emotion: joy\n",
      " > Text splitted to sentences.\n",
      "['I am so happy today!']\n",
      " > Processing time: 1.4029889106750488\n",
      " > Real-time factor: 0.6711190879986295\n",
      "Generated speech with emotion 'joy' saved as 'static/audio/output_joy.wav'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'static/audio/output_joy.wav'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_emotional_speech(\"I am so happy today!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Detected Emotion: sadness\n",
      " > Text splitted to sentences.\n",
      "['I am super sad']\n",
      " > Processing time: 1.2874126434326172\n",
      " > Real-time factor: 0.675891637802124\n",
      "Generated speech with emotion 'sadness' saved as 'static/audio/output_sadness.wav'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'static/audio/output_sadness.wav'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_emotional_speech(\"I am super sad\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
